{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import keras\n",
    "import re\n",
    "from numpy import asarray\n",
    "from sklearn import random_projection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Embedding, Dense, Dropout, Conv2D, MaxPool2D, Concatenate, Input, Reshape, Flatten\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# -------------- Helper Functions --------------\n",
    "def tokenize(text):\n",
    "    tokens = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and not word.isnumeric():\n",
    "            if re.search('[a-zA-Z]',word):\n",
    "                tokens.append(stemmer.stem(word))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_sequence(data, seq_length, vocab_dict):\n",
    "    data_matrix = np.zeros((len(data), seq_length), dtype=int)\n",
    "    for i, doc in enumerate(data):\n",
    "        for j, word in enumerate(doc):\n",
    "            if j == seq_length:\n",
    "                break\n",
    "            word_idx = vocab_dict.get(word, 1) # 1 means the unknown word\n",
    "            data_matrix[i, j] = word_idx\n",
    "\n",
    "    return data_matrix\n",
    "\n",
    "\n",
    "def read_data(file_name, input_length):\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(df['text'])\n",
    "    sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "    data_matrix = pad_sequences(sequences,maxlen=input_length) \n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    # df['words'] = df['text'].apply(tokenize)\n",
    "    # if vocab is None:\n",
    "    #     vocab = set()\n",
    "    #     for i in range(len(df)):\n",
    "    #         for word in df.iloc[i]['words']:\n",
    "    #             vocab.add(word)\n",
    "    # vocab_dict = dict()\n",
    "    # vocab_dict['<pad>'] = 0 # 0 means the padding signal\n",
    "    # vocab_dict['<unk>'] = 1 # 1 means the unknown word\n",
    "    # vocab_size = 2\n",
    "    # for v in vocab:\n",
    "    #     vocab_dict[v] = vocab_size\n",
    "    #     vocab_size += 1\n",
    "    # data_matrix = get_sequence(df['words'], input_length, vocab_dict)\n",
    "    stars = df['stars'].apply(int) - 1\n",
    "    return df['review_id'], stars, data_matrix, word_index\n",
    "\n",
    "\n",
    "def load_data(input_length):\n",
    "    # Load training data and vocab\n",
    "    train_id_list, train_data_label, train_data_matrix, word_index = read_data(\"../yelp-review-dataset/train.csv\", input_length)\n",
    "    K = max(train_data_label)+1  # labels begin with 0\n",
    "\n",
    "    # Load valid data  whether valid has vocab\n",
    "    valid_id_list, valid_data_label, valid_data_matrix, _ = read_data(\"../yelp-review-dataset/valid.csv\", input_length)\n",
    "\n",
    "    # Load testing data\n",
    "    test_id_list, _, test_data_matrix, _ = read_data(\"../yelp-review-dataset/test.csv\", input_length)\n",
    "    \n",
    "    print(\"Vocabulary Size:\", len(word_index))\n",
    "    print(\"Training Set Size:\", len(train_id_list))\n",
    "    print(\"Validation Set Size:\", len(valid_id_list))\n",
    "    print(\"Test Set Size:\", len(test_id_list))\n",
    "    print(\"Training Set Shape:\", train_data_matrix.shape)\n",
    "    print(\"Validation Set Shape:\", valid_data_matrix.shape)\n",
    "    print(\"Testing Set Shape:\", test_data_matrix.shape)\n",
    "\n",
    "    # Converts a class vector to binary class matrix.\n",
    "    # https://keras.io/utils/#to_categorical\n",
    "    train_data_label = keras.utils.to_categorical(train_data_label, num_classes=K)\n",
    "    valid_data_label = keras.utils.to_categorical(valid_data_label, num_classes=K)\n",
    "    return  train_data_matrix, train_data_label, \\\n",
    "         valid_data_matrix, valid_data_label, \\\n",
    "        test_id_list, test_data_matrix, word_index\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_length = 300\n",
    "    embedding_size = 100\n",
    "    hidden_size = 100\n",
    "    batch_size = 200\n",
    "    dropout_rate = 0.2\n",
    "    filters = 100\n",
    "    kernel_sizes = [3, 4, 5]\n",
    "    padding = 'valid'\n",
    "    activation = 'relu'\n",
    "    # strides = 1\n",
    "    strides = embedding_size\n",
    "    pool_size = 2\n",
    "    learning_rate = 0.01\n",
    "    total_epoch = 50\n",
    "\n",
    "    train_data_matrix, train_data_label, valid_data_matrix, \\\n",
    "    valid_data_label,test_data_matrix, test_data_label, word_index = load_data(input_length)\n",
    "\n",
    "    # Data shape\n",
    "    N = train_data_matrix.shape[0]\n",
    "    K = train_data_label.shape[1]\n",
    "\n",
    "    input_size = len(word_index) \n",
    "    output_size = K\n",
    "\n",
    "    # load the whole embedding into memory\n",
    "    # embeddings_index = dict()\n",
    "    # f = open('glove.6B.100d.txt','rb')\n",
    "    # for line in f:\n",
    "    #     values = line.split()\n",
    "    #     word = values[0]\n",
    "    #     coefs = asarray(values[1:], dtype='float32')\n",
    "    #     embeddings_index[word] = coefs\n",
    "    # f.close()\n",
    "    # print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "    # # # create a weight matrix for words in training docs\n",
    "    # embedding_matrix = np.zeros((input_size, embedding_size))\n",
    "    # for word, i in word_index.items():\n",
    "    #     embedding_vector = embeddings_index.get(word)\n",
    "    #     if embedding_vector is not None:\n",
    "    #         embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    \n",
    "    # New model\n",
    "    x = Input(shape=(input_length, ))\n",
    "    print('x',x)\n",
    "    # embedding layer and dropout\n",
    "    # e = Embedding(input_dim=input_size, output_dim=embedding_size,input_length=input_length)(x)\n",
    "    e = Embedding(input_dim=input_size, \n",
    "                output_dim=embedding_size, \n",
    "                weights=[embedding_matrix],\\\n",
    "                input_length=input_length,\n",
    "                trainable=False)(x)\n",
    "    e_d = Dropout(dropout_rate)(e)\n",
    "    # print('e_d',e_d)\n",
    "\n",
    "    # construct the sequence tensor for CNN\n",
    "    e_d = Reshape((input_length, embedding_size, 1))(e_d)\n",
    "    # print('new e_d',e_d)\n",
    "\n",
    "    # CNN layers\n",
    "    conv_blocks = []\n",
    "    for kernel_size in kernel_sizes:\n",
    "        conv = Conv2D(filters=filters, kernel_size=(kernel_size, embedding_size), \n",
    "            padding=padding, activation=activation, strides=(strides, strides))(e_d)\n",
    "        maxpooling = MaxPool2D(pool_size=((input_length-kernel_size)//strides+1, 1))(conv)\n",
    "        faltten = Flatten()(maxpooling)\n",
    "        conv_blocks.append(faltten)\n",
    "\n",
    "    # concatenate CNN results\n",
    "    c = Concatenate()(conv_blocks) if len(kernel_sizes) > 1 else conv_blocks[0]\n",
    "    c_d = Dropout(dropout_rate)(c)\n",
    "\n",
    "    # dense layer\n",
    "    d = Dense(hidden_size, activation=activation)(c_d)\n",
    "\n",
    "    # output layer\n",
    "    y = Dense(output_size, activation='softmax')(d)\n",
    "\n",
    "    # build your own model\n",
    "    model = Model(x, y)\n",
    "    \n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(monitor='loss',min_delta=0,patience=2,verbose=1, mode='auto')\n",
    "\n",
    "    # hyperparameter \n",
    "    dacay = [1e-6]\n",
    "    lr = [0.0005,0.002,0.01]\n",
    "    max_valid_score = 0\n",
    "    best_lr = 0 \n",
    "    best_dacay = 0\n",
    "    for i in lr:\n",
    "        for j in dacay:\n",
    "            adam = keras.optimizers.Adam(lr=i, beta_1=0.9, beta_2=0.999, epsilon=None, decay=j, amsgrad=False)\n",
    "            # sgd = SGD(lr=i, decay=j, momentum=0.9, nesterov=True)\n",
    "\n",
    "            # compile model\n",
    "            model_lstm.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "            \n",
    "            # # training\n",
    "            model_lstm.fit(train_data_matrix, train_data_label, validation_data=(valid_data_matrix,valid_data_label),\n",
    "                epochs=total_epoch, batch_size=batch_size,callbacks=[early_stopping])\n",
    "            # # testing\n",
    "            train_score = model_lstm.evaluate(train_data_matrix, train_data_label, batch_size=batch_size)\n",
    "            print('lr: {}\\n decay: {}\\n Training Loss: {}\\n Training Accuracy: {}\\n'.format(i, j, train_score[0], train_score[1]))\n",
    "            valid_score = model_lstm.evaluate(valid_data_matrix, valid_data_label, batch_size=batch_size)\n",
    "            print('lr: {}\\n decay: {}\\n Validation Loss: {}\\n Validation Accuracy: {}\\n'.format(i, j, valid_score[0], valid_score[1]))\n",
    "            if valid_score[1] > max_valid_score:\n",
    "                max_valid_score = valid_score[1]\n",
    "                best_lr = i\n",
    "                best_dacay = j\n",
    "    print ('best learning rate:{}\\n best decay rate: {}\\n'.format(best_lr,best_dacay))\n",
    "    # predicting\n",
    "    test_pre = model_lstm.predict(test_data_matrix, batch_size=batch_size).argmax(axis=-1) + 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
